{
  "config": {
    "d_model": 192,
    "n_heads": 2,
    "n_layers": 3,
    "d_ff": 512,
    "dropout": 0.2,
    "attention_type": "hybrid",
    "chunk_size": 16,
    "batch_size": 16
  },
  "val_loss": 6.195689644400529,
  "perplexity": 490.6296883681052
}