# mutated_attention.py
# Improved EvoMultiheadSelfAttention class generated by ChatGPT

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Literal


class EvoMultiheadSelfAttention(nn.Module):
    """
    Multi-head self-attention with three algorithms:
      - 'full': standard full causal attention over T (uses SDPA when available)
      - 'chunked': local causal attention with a sliding window of size chunk_size
      - 'hybrid': learned gate between full and chunked attention
    """

    def __init__(
        self,
        d_model: int,
        n_heads: int,
        attention_type: Literal["full", "chunked", "hybrid"] = "full",
        chunk_size: int = 64,
        dropout: float = 0.1,
    ):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        assert d_model % n_heads == 0

        assert attention_type in ("full", "chunked", "hybrid")
        self.attention_type = attention_type
        self.chunk_size = chunk_size

        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

        # Cached causal mask for full attention (reused across forwards)
        self.register_buffer("_causal_mask", None, persistent=False)

        if self.attention_type == "hybrid":
            # Learned scalar gate per layer: sigma(g) ~ how much to trust full attention
            self.hybrid_gate = nn.Parameter(torch.tensor(0.0))

    def forward(self, x: torch.Tensor, causal: bool = True) -> torch.Tensor:
        B, T, _ = x.size()
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)

        # (B, T, H, D) -> (B, H, T, D)
        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)

        if self.attention_type == "full":
            out = self._full_attention(q, k, v, causal)
        elif self.attention_type == "chunked":
            out = self._chunked_attention(q, k, v, causal)
        else:  # 'hybrid'
            full_out = self._full_attention(q, k, v, causal)
            chunk_out = self._chunked_attention(q, k, v, causal)
            gate = torch.sigmoid(self.hybrid_gate)  # scalar in (0,1)
            # Broadcast gate over (B, H, T, D)
            out = gate * full_out + (1.0 - gate) * chunk_out

        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)
        out = self.out_proj(out)
        return out

    def _get_causal_mask(self, T: int, device: torch.device) -> torch.Tensor:
        """
        Returns a cached upper-triangular causal mask of shape (1, 1, T, T)
        with True in positions that should be masked (future positions).
        """
        mask = self._causal_mask
        if (
            mask is None
            or mask.size(-1) < T
            or mask.device != device
        ):
            new_mask = torch.triu(
                torch.ones(T, T, device=device, dtype=torch.bool),
                diagonal=1,
            )
            # Shape to (1, 1, T, T) for broadcasting over (B, H, T, T)
            new_mask = new_mask.view(1, 1, T, T)
            self._causal_mask = new_mask
            mask = new_mask
        return mask[..., :T, :T]

    def _full_attention(self, q, k, v, causal: bool):
        B, H, T, D = q.size()
        device = q.device

        # Fast path: use PyTorch's scaled_dot_product_attention when available.
        try:
            import torch.nn.functional as F

            if hasattr(F, "scaled_dot_product_attention"):
                dropout_p = self.dropout.p if self.training else 0.0
                return F.scaled_dot_product_attention(
                    q,
                    k,
                    v,
                    attn_mask=None,
                    dropout_p=dropout_p,
                    is_causal=causal,
                )
        except (ImportError, AttributeError, TypeError):
            # Fall back to manual implementation below.
            pass

        # Manual scaled dot-product attention.
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(D)  # (B, H, T, T)
        if causal:
            mask = self._get_causal_mask(T, device)  # (1, 1, T, T)
            scores = scores.masked_fill(mask, float("-inf"))

        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        out = torch.matmul(attn, v)  # (B, H, T, D)
        return out

    def _chunked_attention(self, q, k, v, causal: bool):
        """
        Sliding-window local attention.

        Each position attends to at most `chunk_size` previous tokens (including itself).
        This is strictly causal when `causal=True`. For non-causal use, we fall back
        to full attention for simplicity and correctness.
        """
        B, H, T, D = q.size()

        # If not causal, or if the window covers the whole sequence, just use full attention.
        window_size = min(self.chunk_size, T)
        if not causal or window_size >= T:
            return self._full_attention(q, k, v, causal)

        # Flatten heads into batch for efficient computation: (B * H, T, D)
        Bh = B * H
        q_flat = q.reshape(Bh, T, D)
        k_flat = k.reshape(Bh, T, D)
        v_flat = v.reshape(Bh, T, D)

        device = q.device

        # Build sliding window indices for each time step.
        # For each t, indices cover [max(0, t - window_size + 1), ..., t].
        positions = torch.arange(T, device=device)  # (T,)
        window = torch.arange(window_size, device=device)  # (W,)

        # positions[t] - (window_size - 1 - window) gives range t - (W-1) ... t
        rel_idx = positions.unsqueeze(1) - (window_size - 1 - window).unsqueeze(0)  # (T, W)
        invalid = rel_idx < 0  # (T, W) -> positions before start of sequence
        rel_idx_clamped = rel_idx.clamp(min=0)  # Clamp for safe gather

        # Expand indices for batched gather.
        # (Bh, T, W)
        gather_idx = rel_idx_clamped.unsqueeze(0).expand(Bh, -1, -1)
        # (Bh, T, W, D)
        gather_idx_expanded = gather_idx.unsqueeze(-1).expand(-1, -1, -1, D)

        # Expand K/V for gather along the sequence dimension.
        # k_expanded: (Bh, T, W, D)
        k_expanded = k_flat.unsqueeze(2).expand(-1, -1, window_size, -1)
        v_expanded = v_flat.unsqueeze(2).expand(-1, -1, window_size, -1)

        k_windows = torch.gather(k_expanded, 1, gather_idx_expanded)  # (Bh, T, W, D)
        v_windows = torch.gather(v_expanded, 1, gather_idx_expanded)  # (Bh, T, W, D)

        # Query windows: (Bh, T, 1, D)
        q_windows = q_flat.unsqueeze(2)

        # Compute local attention scores: (Bh, T, W)
        scores = torch.matmul(
            q_windows,  # (Bh, T, 1, D)
            k_windows.transpose(-2, -1),  # (Bh, T, D, W)
        ).squeeze(2)  # -> (Bh, T, W)
        scores = scores / math.sqrt(D)

        # Mask out positions that are before the start of the sequence.
        if invalid.any():
            # Broadcast to (Bh, T, W)
            invalid_mask = invalid.unsqueeze(0).expand(Bh, -1, -1)
            scores = scores.masked_fill(invalid_mask, float("-inf"))

        # No need for an explicit future mask: by construction, each window only
        # contains indices <= current position t, so the attention is causal.

        attn = torch.softmax(scores, dim=-1)  # (Bh, T, W)
        attn = self.dropout(attn)

        # Weighted sum of values in the window: (Bh, T, D)
        out_flat = torch.einsum("btw,btwd->btd", attn, v_windows)

        # Reshape back to (B, H, T, D)
        out = out_flat.view(B, H, T, D)
        return out

